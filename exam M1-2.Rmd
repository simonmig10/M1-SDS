---
title: "Exam"
author: "Simon"
date: "22/9/2021"
output: html_document
---


```{r, message = FALSE}
library(tidyverse)

library(lubridate)

library(magrittr)

library(FactoMineR)

library(factoextra)

library(uwot)

library(GGally)

library(rsample)

library(ggridges)

library(xgboost)

library(recipes)

library(parsnip)

library(glmnet)

library(tidymodels)

library(skimr)

library(VIM)

library(visdat)

library(ggmap)

library(ranger)

library(vip)
```


# load data

```{r}
data_start <- read_csv("startup data.csv", 
col_types = cols(founded_at = col_date(format = "%m/%d/%Y"), 
first_funding_at = col_date(format = "%m/%d/%Y"), 
last_funding_at = col_date(format = "%m/%d/%Y")))
```

# data cleaning / EDA


### Format data
Prøver lige en ny visualisering. Først, hvad type er dataet
```{r}
vis_dat(data_start)
```

Hvad med NA's?
```{r}
vis_miss(data_start)
```

Alternativt, i tekst i stedet
```{r}
is.na(data_start) %>% colSums()
```


Fjerner lige de observationer, der ikke ligger i USA.
```{r}
data_start %>% 
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(color = "cornflowerblue")

data_start %<>%
  filter(longitude < -40)
```

We can look how companies perform geographically 

```{r}
qmplot(x = longitude, 
       y = latitude, 
       data = data_start, 
       geom = "point", 
       color = status, 
       size = funding_total_usd, #har lige kørt efter denne variabel så vi kan se hvor de store ligger
       alpha = 0.4) +
  scale_alpha(guide = 'none') # don't show legend for alpha
```
Tror myb der er NA's  i det data her ved ik om det does something

We take a quick look at the data
```{r}
skim(data_start)
```


We start by removing columns there are undefined when loading the dataset and columns which show the same things. We also remove variables like longtitude and latitude, which could be fun to work with later, but doesnt seem to bring much insight to the ongoing analysis. 

```{r}

data = data_start %>%
  select(!c(`Unnamed: 0`, `Unnamed: 6`, state_code.1, object_id, avg_participants, has_roundA, has_roundB, has_roundC, has_roundD, zip_code, id, city, name, latitude, longitude, labels))

```

The data contains 33 numerical variables after the first cleaning, but 22 of those are dummy variables, which are variables we later on could try and see if our clusters are separated by. But as over main objective is to try and separate by the status column "closed or "acquired" we will now take a look an see, which variables seems to be correlated with the status column, starting with the jobtype related dummies, the state related dummies and lastly the three remaining dummies.

```{r, warning=FALSE}
jobs = data %>% 
  select(is_software, is_web, is_mobile, is_enterprise, is_advertising, is_gamesvideo, is_ecommerce, is_biotech, is_consulting, is_othercategory, status)

state = data %>% 
  select(is_CA, is_TX, is_MA, is_NY, is_otherstate, status)

dummies = data %>%
  select(is_top500, has_angel, has_VC, status)

par(mfrow=c(1,3))
jobs %>%
  select(status, is_numeric) %>%
  gather(variable, value, -status) %>%
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(status), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)

state %>%
  select(status, is_numeric) %>%
  gather(variable, value, -status) %>%
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(status), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)

dummies %>%
  select(status, is_numeric) %>%
  gather(variable, value, -status) %>%
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(status), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)

```

The plot of the job type related dummies shows, that there is almost no differences in how many firm closes and how many who becomes acquired according to job type.
  Looking at the state related dummies, the plot shows that a bit more firms from New York, Texas and California are being acquired than from the rest of the united states, which could indicate that they could be useful variables to use in our analysis. But you could also say, that the differences aren't big which would make them less influential in the analysis.
   Lastly in the plot of the three remaining dummy variables it is clear to see that whether or not the firm has been a top 500 upcoming firm substantially change the chance of closing or not. Whether they have VC or angel investor doesn't seem to influence the outcome in a big way. 

Because of the above we drop the state and job variables plus the VC and angel dummies, but we keep is_top200 and then we skim the data again.

```{r}
data %<>%
  select(-c(is_CA, is_TX, is_MA, is_NY, is_otherstate, is_software, is_web, is_mobile, is_enterprise, is_advertising, is_gamesvideo, is_ecommerce, is_biotech, is_consulting, is_othercategory, state_code, category_code, has_angel, has_VC))

skim(data)

```
We still have 8 numerical variables and 3 date variables we haven't taken a closer look at so that is what we gonna do now.

```{r, warning=FALSE}

data %>%
  select(status, is_numeric, !is_top500) %>%
  gather(variable, value, -status, -is_top500) %>%
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(status), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)

```

The above plot shows, that relationships, milestones, funding_total_usd, funding_rounds, closed at, both first and last milestone_year and last_funding_year seems to impact whether or not a firm gets acquired or not. 
    Closed_at will not be used because it contains almost 600 missing observations which is close to 66% of all observations. Founded at will not be used either simply because there doesnt seem to be that big of a difference in the density plots. And using both first and last milestone_year would maybe be irrelevant because they somehow nearly show the same thing. so we drop them. First and last funding_at doesn't show any significance either, so those also gets dropped.
    
```{r}
data%<>%
  select(!c(age_last_milestone_year, closed_at, age_first_funding_year,founded_at, first_funding_at, last_funding_at))
```

Now we are left with only our status variable and our 7 numerical variables of interest. Now we check for missing values in our remaning variables.

```{r}
data %>%
  select(everything()) %>%
  summarise_all(funs(sum(is.na(.))))
```

The above states that the age_first_milestone_year has 152 missing values. THe problem with this variable is, that if we remove those rows, we lose 16.4% of our observations, and we cant just replace the NA's with something else like zero, because that would just manipulate our data. So the solution we have come up with is to make them into intervals and then just pick one of the intervals to be used in our analysis. 

```{r}

#We make intervals for first milestone
data %<>%
  mutate(age_first_milestone_year= ifelse(age_first_milestone_year <0,"before year 0",ifelse(age_first_milestone_year <= 3 , "[0-3]", ifelse(age_first_milestone_year <= 6, "]3-6]", ifelse(age_first_milestone_year >6, "over 6",""))))) %>%
  mutate(age_first_milestone_year= replace_na(age_first_milestone_year, "no milestone"))

data %<>% 
  mutate("is_before_start" = as.numeric(age_first_milestone_year == "before year 0")) %>% 
  mutate("is_0:3" = as.numeric(age_first_milestone_year == "[0-3]")) %>% 
  mutate("is_3:6" = as.numeric(age_first_milestone_year == "]3-6]")) %>%
  mutate("is_6<" = as.numeric(age_first_milestone_year == "over 6")) %>% 
  mutate("is_no_milestone" = as.numeric(age_first_milestone_year == "no milestone"))

data %<>%
  select(!c(age_first_milestone_year, is_before_start, `is_3:6`, `is_6<`, is_no_milestone))

```

We pick the interval zero to three years, so we drop the rest, which means that firms achieved their first milestone when they were between 0 and 3 years old. We pick this interval because we wanna investigate whether it is positive or not in terms of getting acquired for a firm to get their first milestone quick eg. in the first couple of years.

And this conclude our variable selection which now will be explained more thoroughly below.

```{r}
names = colnames(data)
description = c("numeric, age at last/latest funding", "numeric, number of relationships", "numeric, number of funding rounds", "numeric, the total funding in USD", "numeric, number og milestones", "dummy variable, has been a top 500 start up", "character string, acquired or closed", "reach their first milestone at the age of 0-3 years")

table = cbind(names, description); table
```
Now we move on to unsupervised machine learning.

# Unsupervised ML

We start by performing dimensionalty reduction on our data, but before we can do that, we need to exmine or data to figure out if we need to scale it?

```{r}
options(scipen = 999)

data_num = data %>% 
  select_if(is.numeric)

s_deviation=apply(data_num,2, sd)

mean1=colMeans(data_num)

scale= as.data.frame(s_deviation, row.names = c("sd"))%>%
cbind(as.data.frame(mean1, row.names = "mean"))%>%
print()
```
We can se the data is gonna need scaling to peform pca, because our variables is not on the same scale. Namely funding_total_usd seems much higher. We also have some dummy variables which only take a value of either 0 or 1, so these will also be on a different scale then the rest, which is makes the case of scaling the data even stronger.
   So we scale the data by setting the argument scale.unit to TRUE and then we run the PCA. Further we can only run our PCA on numeric variables so we use the data subset just created above "data_num".

```{r warning=FALSE}
res_pca <- data_num %>%
  PCA(scale.unit = TRUE, graph =FALSE)

```

Now we have our PCA we can create a screeplot to pick number og dimensions to use. 
```{r}
res_pca %>% 
  fviz_screeplot(addlabels = TRUE, 
                 ncp = 10, 
                 ggtheme = theme_gray())

eig.val = get_eigenvalue(res_pca); eig.val
```
We can see the elbow shows the optimal dimensions are three dimensions with almost 65% explained variance. If we only look at the eigenvalues then our rule of thumb is to pick the dimensions with an eigenvalue >=1, which in this case is two dimensions. Those two dimensions only account for 52% of the total variance, which isn't that high.
   As it is hard to understand a plot with three dimensions we visualize our reduced data in two dimensions.

```{r}
res_pca %>%
  fviz_pca_var(alpha.var = "cos2",
               col.var = "contrib",
               gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
               repel = TRUE,
               ggtheme = theme_gray()) 
```

From the plot we can see, that the x-axis split our six variables into two groups. Where the ones involving funding or money is below the x_axis and variables like milestones, relationships and is_0:3 are above the x-axis. Our last variable whether a firm has been top500 lies almost on the x-axis, which indicates that it is mostly explained by the first dimension.
 It can be said about our variables, that the less opaque the arrow the higher the cos2 and thereby a higher representation of the variable in the principal components we have used eg. funding total USD is not as well explained by the first two principal components as the other variables. Contrib show almost the same as it is just (cos2*100)/(total cos2 of the component), that is why we see the one which are the most opaque also being the one with a blue/greenish color. 

Now we plot all our observations in the two dimensional space and try to split them up by there status of either being aquired or closed.
```{r, fig.height=7, fig.width=9, warning=FALSE}
res_pca %>%
fviz_pca_biplot(alpha.ind = "cos2",
habillage = data %>% pull(status) %>% factor(),
addEllipses = TRUE,
geom = "point",
ggtheme = theme_gray())

```

The plot shows that the red which is the firms who have been aquired tends to be more to the right, which indicates that they have reach more milestones, had more relationships and had a higher tendency to be a top500 start up. But to conclude the PCA doesnt really seperate the firms by status.
   Next we are going to use another dimensionality reduction method namely umap to see if it does a better job.
   
## UMAP
First we create the UMAP object and remember to scale it:
```{r}
res_umap <- data_num  %>%
umap(n_neighbors = 15,
     metric = "cosine",
min_dist = 0.01,
scale = TRUE)
```

Then we plot it in a two dimensional space and fill our observations by status:

```{r}
res_umap %>%
as_tibble() %>%
ggplot(aes(x = V1, y = V2, fill = data$status)) +
geom_point(shape = 21, alpha = 0.5)
```

UMAP seem to separate the observations better as we kinda have 4 clusters here. But they are not separated that well between closed and acquired. It does however seem like most of the firms which has been acquired is to the left and the top left of the plot.

## K-means clustering

Now we wanna look at clustering of our data to see, whether our data are clustered by status or not, we make clusters both on our data and on our pca data and we both make K-means clustering and hierachical clustering.
   First K-means clustering on our dataset.
```{r}
data_num %>%
scale() %>%
fviz_nbclust(kmeans, method = "wss")
```
Seems like the optimal number of clusters is somewhere around 4 clusters, but as we wanna show whether we can make two clusters which are separated by status which only has two different values "acquired" and "closed" we move on with two clusters.

```{r}
res_km2 <- data_num %>%
scale() %>%
kmeans(centers = 2, nstart = 20)
```


```{r}
res_km2 %>%
fviz_cluster(data = data_num ,
ggtheme = theme_gray())
```

The two clusters created by the K-means doesn't seem to separate our data points between status, which is unfortunate, but we still take a look of the result in a table by extracting the cluster number and putting into our data set.

```{r}
data[,"cluster2"] <- res_km2$cluster
  
table(data$cluster2, data$status)

```
The clusters don't seem to be separated by the status of the firm at all. We can see that cluster 1 both has most firm who have been acquired and closed.

Now we try to run K-means again this time on the dimensionalty reduced data, so we extracted the first two components and put them into a new data set called pca_data. 


## K-means clustering after dimensionality reduction
```{r}
pca1 = res_pca$ind$coord[,1]
pca2 = res_pca$ind$coord[,2]
data_pca = data.frame(pca1, pca2)


data_pca %>%
  scale() %>%
  fviz_nbclust(kmeans, method = "wss")
```

We can now see the elbow is formed at 3 clusters, but we still move on with 2 as we wanna try and separate by status.

```{r}

res_km_pca1 <- data_pca %>% 
  scale() %>% 
  kmeans(centers = 2, nstart = 20)  
```


```{r,,fig.width=15,fig.height=10,fig.align='center'}
res_km_pca1 %>% 
  fviz_cluster(data = data_pca,
               ggtheme = theme_gray())  

```

Clustering with the pca data seems to make two clusters who are much more separated, so we again check in a table to to see how well they are separated by status

```{r}
data[,"cluster_pca2"] <- res_km_pca1$cluster
  
table(data$cluster_pca2, data$status)
table(data$cluster2, data$status)
```
Running it on the dimensionality reduced data seems to have separated the firms by status a little worse then by running it on the entire data. We see the acquired firms are almost identical between the two, but the number of closed firm in each clusters has come closer to each other instead of farther away.

Now we will try to use a different clustering method namely hierachical clustering.

## Hierarchical clustering

We make the hclust object and put stand equal to true as it will scale our data.
```{r}
res_hc = data_num %>% 
  hcut(hc_func = "hclust", k = 2, stand = TRUE)
```

Then we make a dendogram

```{r}
res_hc %>% 
  fviz_dend(rect = TRUE, cex = 0.5)
```
The dendogram can be a bit of a mess when you deal with a high number of observations, but we can clearly see that by dealing with only two clusters we separate our data high up in the dendogram, which basiclly means that the two clusters are hard to distinguish between, which partly explains why we are having a hard time separating our firms by status.
   But as before we can plot our clusters in two dimensional space
  
```{r}
res_hc %>% 
  fviz_cluster(data = data_num, ggtheme = theme_gray())
```
The plot in it self looks like the two clusters a fairly overlapped which might be due to us forcing 2 clusters on it instead 4 as the screeplot showed, but let us take a look at how well it separated the firms by status.

```{r}
data[,"cluster_hclust2"] <- res_hc$cluster
  
table(data$cluster_hclust2, data$status)
```
The hierarchical clustering of the data seems to do a better job of at least putting the acquired firms in different clusters, although the closed are still fairly spread out.
   Lastly let us try hclust on the PCA data.
   

## Hierarchical clustering after dimensionality reduction

We could do this as we did with K-means but hclust has this build in function who knows where to get the PCA's and then cluster them.
```{r}
res_hcpc = res_pca %>% 
  HCPC(nb.clust = 2, graph = FALSE)
```

Then we plot it
```{r}
res_hcpc %>% 
  plot(choice = "map")
```
The plot is a bit of a mess because it also plot the trees and it is hard to distinguish were the black dots starts and where the red begins, so it seems like the clusters are not ideal, this might again be due to the fact stated above.

```{r}
data[,"cluster_hclust_pca2"] <- res_hcpc$data.clust$clust
  
table(data$cluster_hclust_pca2, data$status)
table(data$cluster_hclust2, data$status)
```
It can be seen that running hierarchical clustering both before and after dimensionality reduction seems to yield almost identical outcomes. Cluster 1 has just become bigger by two closing firms and that is the only difference.
   The variables used in this analysis wasnt able to capture the effects that are crucial to whether a start up is getting acquired or closed. This might be due to that fact that a lot more variables plays a part in this and maybe also a bit of luck, which cant be quantified.


# SML
```{r}
data %>% head()
```

```{r}
data %>% glimpse()

```


```{r}
data_sml= data %>%
  rename(y = status) %>%
  select(y, funding_total_usd, funding_rounds, relationships, milestones, is_top500, `is_0:3`, age_last_funding_year) 

glimpse(data_sml)
```

## Data Preprocessing


### Training & Test split

```{r}
set.seed(123)

data_split <- initial_split(data_sml, prop = 0.75, strata = y)

data_train <- data_split  %>%  training()
data_test <- data_split %>% testing()
```


### EDA on the training data



### Preprocessing recipe


step_log bruges hvis variablen er lid scewed!

```{r}

data_recipe <- data_train %>%
  recipe(y ~.) %>%
  step_log(funding_total_usd) %>% #Myb just delete this one
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  prep()


#na.omit(all_predictors()) %>%


summary(data_recipe)

```



```{r}
prepped_data <- 
  data_recipe %>% # use the recipe object
  prep() %>% # perform the recipe on training data
  juice()%>%
  glimpse()# extract only the preprocessed dataframe
```

### Defining the models



#### Logistic Regression

```{r}
model_lg <- logistic_reg(mode = 'classification') %>%
  set_engine('glm', family = binomial) 
```

#### Decision tree

```{r}
model_dt <- decision_tree(mode = 'classification',
                          cost_complexity = tune(),
                          tree_depth = tune(), 
                          min_n = tune()
                          ) %>%
  set_engine('rpart') 
```

#### Extreme Gradient Boosted Tree (XGBoost)

```{r}
model_xg <- boost_tree(mode = 'classification', 
                       trees = 100,
                       mtry = tune(), 
                       min_n = tune(), 
                       tree_depth = tune(), 
                       learn_rate = tune()
                       ) %>%
  set_engine("xgboost") 
```


### K-nearest neighbor
```{r}
model_knn <- 
  nearest_neighbor(neighbors = 4) %>% # we can adjust the number of neighbors 
  set_engine("kknn") %>% 
  set_mode("classification") 


```


### Random forest
```{r}
model_rf <- 
  rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")


```

#### Define workflow



```{r}
workflow_general <- workflow() %>%
  add_recipe(data_recipe) 

workflow_lg <- workflow_general %>%
  add_model(model_lg)

workflow_dt <- workflow_general %>%
  add_model(model_dt)

workflow_xg <- workflow_general %>%
  add_model(model_xg)

workflow_knn <- workflow_general %>%
  add_model(model_knn)

workflow_rf <- workflow_general %>%
  add_model(model_rf)
```



### Hyperparameter Tuning

#### Validation Sampling (N-fold crossvlidation)



```{r}

set.seed(100)

data_resample <- data_train %>% 
  vfold_cv(strata = y,
           v = 3,
           repeats = 3)
```

#### Hyperparameter Tuning: Decision Tree

Need to find out what the error is here!

```{r}
tune_dt <-
  tune_grid(
    workflow_dt,
    resamples = data_resample,
    grid = 5
  )
```

```{r}
tune_dt %>% autoplot()
```

```{r}
best_param_dt <- tune_dt %>% select_best(metric = 'roc_auc')
best_param_dt
```

```{r}
tune_dt %>% show_best(metric = 'roc_auc', n = 1)
```


#### Hyperparameter Tuning: Random Forest



```{r}

tune_xg <-
  tune_grid(
    workflow_xg,
    resamples = data_resample,
    grid = 10
  )
```

```{r}
tune_xg %>% autoplot()
```

```{r}
best_param_xg <- tune_xg %>% select_best(metric = 'roc_auc')
best_param_xg
```

```{r}
tune_xg %>% show_best(metric = 'roc_auc', n = 1)
```


#### Fit models with tuned hyperparameters




```{r}
workflow_final_dt <- workflow_dt %>%
  finalize_workflow(parameters = best_param_dt)

workflow_final_xg <- workflow_xg %>%
  finalize_workflow(parameters = best_param_xg)
```

## Evaluate models
Her bruger vi vores validation set

### Logistic regression
We use our workflow object to perform resampling. Furthermore, we use metric_set()to choose some common classification performance metrics provided by the yardstick package. Visit yardsticks reference to see the complete list of all possible metrics.

Note that Cohen’s kappa coefficient (κ) is a similar measure to accuracy, but is normalized by the accuracy that would be expected by chance alone and is very useful when one or more classes have large frequency distributions. The higher the value, the better.

```{r}

log_res <- 
  workflow_lg %>% 
  fit_resamples(
    resamples = data_resample, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(
      save_pred = TRUE)
    ) 

log_res %>% collect_metrics(summarize = TRUE)
```


#### Model coefficients

```{r}
# save model coefficients for a fitted model object from a workflow

get_model <- function(x) {
  pull_workflow_fit(x) %>% tidy()
}

# same as before with one exception
log_res_2 <- 
  workflow_lg %>% 
  fit_resamples(
    resamples = data_resample, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(
      save_pred = TRUE,
      extract = get_model) # use extract and our new function
    ) 
```

```{r}
log_res_2$.extracts[[1]][[1]]
```

```{r}
all_coef <- map_dfr(log_res_2$.extracts, ~ .x[[1]][[1]])
```

Show all of the resample coefficients for a single predictor:
```{r}
filter(all_coef, term == "relationships")
```

### Dicision tree


```{r}
dt_res <- 
  workflow_final_dt %>% 
  fit_resamples(
    resamples = data_resample, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

dt_res %>% collect_metrics(summarize = TRUE)
```

### XGboost

```{r}
xgb_res <- 
  workflow_final_xg %>% 
  fit_resamples(
    resamples = data_resample, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

xgb_res %>% collect_metrics(summarize = TRUE)
```

### KNN
```{r}
knn_res <- 
  workflow_knn %>% 
  fit_resamples(
    resamples = data_resample, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

knn_res %>% collect_metrics(summarize = TRUE)
```
### Random forrest

```{r}
rf_res <-
  workflow_rf %>% 
  fit_resamples(
    resamples = data_resample, 
    metrics = metric_set(
      recall, precision, f_meas, 
      accuracy, kap,
      roc_auc, sens, spec),
    control = control_resamples(save_pred = TRUE)
    ) 

rf_res %>%  collect_metrics(summarize = TRUE)
```



```{r}
#fit_lg <- workflow_lg %>%
  #fit(data_train)

#fit_dt <- workflow_final_dt %>%
  #fit(data_train)

#fit_xg <- workflow_final_xg %>%
  #fit(data_train)
```

#### Compare performance

## Compare models

```{r}
log_metrics <- 
  log_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Logistic Regression") # add the name of the model to every row

rf_metrics <- 
  rf_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Random Forest")

xgb_metrics <- 
  xgb_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "XGBoost")

knn_metrics <- 
  knn_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Knn")

dt_metrics <- 
  dt_res %>% 
  collect_metrics(summarise = TRUE) %>%
  mutate(model = "Decision tree")

```


```{r}
# create dataframe with all models
model_compare <- bind_rows(
                          log_metrics,
                           rf_metrics,
                           xgb_metrics,
                           knn_metrics,
                          dt_metrics,
                           ) 

# change data structure
model_comp <- 
  model_compare %>% 
  select(model, .metric, mean, std_err) %>% 
  pivot_wider(names_from = .metric, values_from = c(mean, std_err)) 

# show mean F1-Score for every model
model_comp %>% 
  arrange(mean_f_meas) %>% 
  mutate(model = fct_reorder(model, mean_f_meas)) %>% # order results
  ggplot(aes(model, mean_f_meas, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(mean_f_meas, 2), y = mean_f_meas + 0.08),
     vjust = 1
  )
```
```{r}
# show mean area under the curve (auc) per model
model_comp %>% 
  arrange(mean_roc_auc) %>% 
  mutate(model = fct_reorder(model, mean_roc_auc)) %>%
  ggplot(aes(model, mean_roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") + 
     geom_text(
     size = 3,
     aes(label = round(mean_roc_auc, 2), y = mean_roc_auc + 0.08),
     vjust = 1
  )
```

# choose model

## Log-reg model 

### Performance metrics
Show average performance over all folds (note that we use log_res):

```{r}
log_res %>%  collect_metrics(summarize = TRUE)
```
Show performance for every single fold:

```{r}
log_res %>%  collect_metrics(summarize = FALSE)
```

### Collect model predictions
To obtain the actual model predictions, we use the function collect_predictions and save the result as log_pred:

```{r}
log_pred <- 
  log_res %>%
  collect_predictions()
```

### Confusion Matrix

Nu kan vi bruge vores indsamlede predictions til at lave en confusion matrix

```{r}
log_pred %>% 
  conf_mat(y, .pred_class) 
```

Eller
```{r}
log_pred %>% 
  conf_mat(y, .pred_class) %>% 
  autoplot(type = "mosaic")
```

Eller (den her synes jeg er bedst)

```{r}
log_pred %>% 
  conf_mat(y, .pred_class) %>% 
  autoplot(type = "heatmap")
```

## ROC curve
```{r}
log_pred %>% 
  roc_curve(y, .pred_acquired) %>% 
  autoplot()
```

Med folds
```{r}
log_pred %>% 
  group_by(id) %>%
    roc_curve(y, .pred_acquired) %>% 
  autoplot()
```

Kan vi lave på andre end log


```{r}
log_pred %>% 
  ggplot() +
  geom_density(aes(x = .pred_acquired, 
                   fill = y), 
               alpha = 0.5)
```

## XGboost model 

### Performance metrics
Show average performance over all folds (note that we use log_res):

```{r}
xgb_res %>%  collect_metrics(summarize = TRUE)
```
Show performance for every single fold:

```{r}
xgb_res %>%  collect_metrics(summarize = FALSE)
```

### Collect model predictions
To obtain the actual model predictions, we use the function collect_predictions and save the result as log_pred:

```{r}
xgb_pred <- 
  xgb_res %>%
  collect_predictions()
```

### Confusion Matrix

Nu kan vi bruge vores indsamlede predictions til at lave en confusion matrix

```{r}
xgb_pred %>% 
  conf_mat(y, .pred_class) 
```

Eller
```{r}
xgb_pred %>% 
  conf_mat(y, .pred_class) %>% 
  autoplot(type = "mosaic")
```

Eller (den her synes jeg er bedst)

```{r}
xgb_pred %>% 
  conf_mat(y, .pred_class) %>% 
  autoplot(type = "heatmap")
```

## ROC curve
```{r}
xgb_pred %>% 
  roc_curve(y, .pred_acquired) %>% 
  autoplot()
```

Med folds
```{r}
xgb_pred %>% 
  group_by(id) %>%
    roc_curve(y, .pred_acquired) %>% 
  autoplot()
```

Kan vi lave på andre end log


```{r}
xgb_pred %>% 
  ggplot() +
  geom_density(aes(x = .pred_acquired, 
                   fill = y), 
               alpha = 0.5)
```

## Random forrest model 

### Performance metrics
Show average performance over all folds (note that we use log_res):

```{r}
rf_res %>%  collect_metrics(summarize = TRUE)
```
Show performance for every single fold:

```{r}
rf_res %>%  collect_metrics(summarize = FALSE)
```

### Collect model predictions
To obtain the actual model predictions, we use the function collect_predictions and save the result as log_pred:

```{r}
rf_res <- 
  log_res %>%
  collect_predictions()
```

### Confusion Matrix

Nu kan vi bruge vores indsamlede predictions til at lave en confusion matrix

```{r}
rf_res %>% 
  conf_mat(y, .pred_class) 
```

Eller
```{r}
rf_res %>% 
  conf_mat(y, .pred_class) %>% 
  autoplot(type = "mosaic")
```

Eller (den her synes jeg er bedst)

```{r}
rf_res %>% 
  conf_mat(y, .pred_class) %>% 
  autoplot(type = "heatmap")
```

## ROC curve
```{r}
rf_res %>% 
  roc_curve(y, .pred_acquired) %>% 
  autoplot()
```

Med folds
```{r}
rf_res %>% 
  group_by(id) %>%
    roc_curve(y, .pred_acquired) %>% 
  autoplot()
```

Kan vi lave på andre end log


```{r}
rf_res %>% 
  ggplot() +
  geom_density(aes(x = .pred_acquired, 
                   fill = y), 
               alpha = 0.5)
```

#### We use XGboost on the test data


```{r}
last_fit_xgb <- last_fit(workflow_final_xg, 
                        split = data_split,
                        metrics = metric_set(
                          recall, precision, f_meas, 
                          accuracy, kap,
                          roc_auc, sens, spec)
                        )
```


```{r}
last_fit_xgb %>% 
  collect_metrics()
```


```{r}
last_fit_xgb %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 10)
```

```{r}
last_fit_xgb %>%
  collect_predictions() %>% 
  conf_mat(y, .pred_class) %>% 
  autoplot(type = "heatmap")

```

```{r}
last_fit_xgb %>% 
  collect_predictions() %>% 
  roc_curve(y, .pred_acquired) %>% 
  autoplot()
```

## Andet test data myb slet? 

```{r}
pred_collected_t <- tibble(
  truth = data_test %>% pull(y) %>% as.factor(),
  #base = mean(truth),
  lg = fit_lg %>% predict(new_data = data_test) %>% pull(.pred_class),
  dt = fit_dt %>% predict(new_data = data_test) %>% pull(.pred_class),
  xg = fit_xg %>% predict(new_data = data_test) %>% pull(.pred_class),
  ) %>% 
  pivot_longer(cols = -truth,
               names_to = 'model',
               values_to = '.pred')
```


```{r}

#Logistic regression

pred_lg=pred_collected_t %>%
  filter(model== "lg")%>%
  pull(.pred)

truth_lg=pred_collected_t %>%
  filter(model== "lg")%>%
  pull(truth)


caret::confusionMatrix(data = pred_lg, reference= truth_lg )

#Decision tree

pred_dt=pred_collected_t %>%
  filter(model== "dt")%>%
  pull(.pred)

truth_dt=pred_collected_t %>%
  filter(model== "dt")%>%
  pull(truth)

caret::confusionMatrix(data = pred_dt, reference= truth_dt )


#XG boost

pred_xg=pred_collected_t %>%
  filter(model== "xg")%>%
  pull(.pred)

truth_xg=pred_collected_t %>%
  filter(model== "xg")%>%
  pull(truth)

caret::confusionMatrix(data = pred_xg, reference= truth_xg )

```























