---
title: "R Notebook"
output: html_notebook
---

#Before we start

Running libaries 

```{r message=FALSE}
library(tidyverse)

library(lubridate)

library(magrittr)

library(FactoMineR)

library(factoextra)

library(uwot)

```


```{r}
pokemon= read_csv("https://sds-aau.github.io/SDS-master/00_data/pokemon.csv")
```

#Tasks

**Give a brief overview of data, what variables are there, how are the variables scaled and variation of the data columns.**

```{r}
pokemon %>% head()
```

```{r}
pokemon %>% glimpse()




pokemon$Type2 %<>%
  replace_na("No 2. type")

```



```{r}
pokemon %>% count(Type1, sort = TRUE)

pokemon %>% count(Type2, sort = TRUE)
```



*Character strings*
We can see the data has 3 character strings "names" which are the names of the pokemons, "type1" which is the main type of the pokemon, and last "type2" which shows some pokemons has a second type, we guess NA's means they only have 1 type. 


*Numeric values*
We can see the first column just counts the pokemons (ID) so we remove this 

```{r}
pokemon %<>% select(!Number)

glimpse(pokemon)

```


```{r}
pokemon %>%
  count(Generation)
```
We can see Generation is numeric but seems like it shows some kind of categorical variable. 


```{r warning=FALSE}

pokemon %<>%
  drop_na()

pokemon_sd= pokemon %>%
  select(HitPoints, Attack, Defense, SpecialAttack, SpecialDefense, Speed, Total)

s_deviation=apply(pokemon_sd, 2, sd)

mean=colMeans(pokemon_sd)

pokemon_stats= as.data.frame(s_deviation, row.names = c("sd"))%>%
  cbind(as.data.frame(mean, row.names = "mean"))%>%
  print()



```
 When we look at the other numerical values we have calculated the standard deviation and the mean, to see if the data is of the same scaling, this shows that Total has a much larger standard deviation and mean. Else the other variables are almost the same. But becasue of the Total variable we should scale the data. 

*logical values*

We can see the Legendary column is a logical variable, which means FALSE observation shows pokemons whom are not legendary, and true for those who are legendary. This variable can not be scaled. 


**Execute a PCA analysis on all numerical variables in the dataset. Hint: Don’t forget to scale them first. Use 4 components. What is the cumulative explained variance ratio? Hint: I am not sure this terminology and code was introduced during class, but try and look into cumulative explained variance and sklearn(package) and see if you can figure out the code needed.**

```{r warning=FALSE}


res_pca <- pokemon %>%
  select(!Generation)%>%
  select_if(is_numeric) %>%
  PCA(scale.unit = TRUE, graph =FALSE)


```

```{r}
res_pca %>% 
  fviz_screeplot(addlabels = TRUE, 
                 ncp = 10, 
                 ggtheme = theme_gray())
```

We can see the albow shows the optimal dimension is 2 dimensions. 

```{r}
res_pca %>%
  fviz_pca_var(alpha.var = "cos2",
               col.var = "contrib",
               gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
               repel = TRUE,
               ggtheme = theme_gray()) 
```

Let's visualize our observations and the variable-loading together in the space of the first 2 components.

```{r,,fig.width=15,fig.height=10,fig.align='center'}
res_pca %>%
  fviz_pca_biplot(alpha.ind = "cos2",
                  col.ind = "contrib",
                  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                  geom = "point", 
                  ggtheme = theme_gray())
  
                  
```
From the 2 plots we can see that some of the variables are correlated fx. Attack and Total seems to be very much correlated the distance between the lines shows the correlation, We can see that speed and Defence has a very low correlation as the angle is almost 90 degrees. We can also see there are no negative correlation between the variables, as there are no angels above 90 degrees. 


```{r}
res_pca$eig[,3][1:4]
```

We can see that the cumulative variance at component 4 is 90.05% This means the 4 dimensions explain 90.05% of the variance in the data. 


**Use a different dimensionality reduction method (eg. UMAP/NMF) – do the findings differ?**

```{r}
res_umap <- pokemon %>%
  select(!Generation)%>%
  select_if(is_numeric) %>%
  umap(n_neighbors = 15, 
       metric = "cosine", 
       min_dist = 0.01, 
       scale = TRUE) 
```

```{r}
res_umap %>% as_tibble() %>%
  glimpse()
```
```{r}

res_umap %>%
  as_tibble() %>%
  ggplot(aes(x = V1, y = V2, fill = pokemon$Generation)) + 
  geom_point(shape = 21, alpha = 0.5)

```
The conclusion is that We make a conclusion  later


**Perform a cluster analysis (KMeans) on all numerical variables (scaled & before PCA). Pick a realistic number of clusters (up to you where the large clusters remain mostly stable).**

We create 

```{r}

pokemon %>%
  select(!Generation)%>%
  select_if(is_numeric) %>% 
  scale() %>%
  fviz_nbclust(kmeans, method = "wss")  

#manuelt

pokemon_nr=pokemon%>%
  select(!Generation)%>%
  select_if(is_numeric)

wss <- 0
for (i in 1:7) {
  km.out <- kmeans(pokemon_nr, centers = i, nstart = 20)
  # Save total within sum of squares to wss variable
  wss[i] <- km.out$tot.withinss
}

plot(1:7, wss, type = "b", 
     xlab = "Number of Clusters", 
     ylab = "Within groups sum of squares")



```

We can see from both plots that it looks like we should use 2 clusters.  


```{r}
res_km <- pokemon_nr %>% 
  select_if(is_numeric) %>%
  scale() %>% 
  kmeans(centers = 3, nstart = 20)  

```
```{r}
res_km 
```

```{r,,fig.width=15,fig.height=10,fig.align='center'}
res_km %>% 
  fviz_cluster(data = pokemon_nr %>% select_if(is_numeric) ,
               ggtheme = theme_gray())  
```





**Visualize the first 2 principal components and color the datapoints by cluster.**

```{r}
pokemon_nr[,"pca1"] <- res_pca$ind$coord[,1]
pokemon_nr[,"pca2"] <- res_pca$ind$coord[,2]

glimpse(pokemon_nr)

pokemon_cluster=pokemon
pokemon_cluster_nr=pokemon_nr

pokemon_cluster[,"cluster"] <- res_km$cluster
pokemon_cluster_nr[,"cluster"] <- res_km$cluster

pokemon_cluster_nr %>%
  ggplot(aes(x=pca1, y=pca2, color=factor(cluster)))+
  geom_point()

```



**Inspect the distribution of the variable Type1 across clusters. Does the algorithm separate the different types of pokemon?**


```{r}

type1_table=table(pokemon_cluster$cluster, pokemon_cluster$Type1)

```
We can see that the algorithm do not fully separate the different types of pokemons into the 3 clusters.

**Perform a cluster analysis on all numerical variables scaled and AFTER dimensionality reduction and visualize the first 2 principal components.**


```{r}

pokemon_pca= pokemon_nr%>%
  select(pca1, pca2)

pokemon_pca %>%
  scale() %>%
  fviz_nbclust(kmeans, method = "wss")
```


```{r}

res_km_pca <- pokemon_pca %>% 
  scale() %>% 
  kmeans(centers = 3, nstart = 20)  
```

```{r}
res_km_pca 
```

```{r,,fig.width=15,fig.height=10,fig.align='center'}
res_km_pca %>% 
  fviz_cluster(data = pokemon_pca,
               ggtheme = theme_gray())  
```
```{r}

pokemon_cluster_pca= pokemon
pokemon_cluster_nr_pca= pokemon_nr

pokemon_cluster_pca[,"cluster_pca"] <- res_km_pca$cluster
pokemon_cluster_nr_pca[,"cluster_pca"] <- res_km_pca$cluster

pokemon_cluster_nr_pca %>%
  ggplot(aes(x=pca1, y=pca2, color=factor(cluster_pca)))+
  geom_point()

```

**Again, inspect the distribution of the variable “Type 1” across clusters, does it differ from the distribution before dimensionality reduction?**

```{r}

table(pokemon_cluster_pca$cluster_pca, pokemon_cluster_pca$Type1)



```

```{r}
type1_table
```
It seems like the clusters are categorized in relation to attributes or abilities, and not so much the type of the pokemon. 



