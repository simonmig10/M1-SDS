```{r}
library(tidyverse)

library(lubridate)

library(magrittr)

library(FactoMineR)

library(factoextra)

library(uwot)

library(GGally)

library(rsample)

library(ggridges)

library(xgboost)

library(recipes)

library(parsnip)

library(glmnet)

library(tidymodels)

library(skimr)

library(VIM)
```


# load data

```{r}
data <- read_csv("startup data.csv", 
col_types = cols(founded_at = col_date(format = "%m/%d/%Y"), 
first_funding_at = col_date(format = "%m/%d/%Y"), 
last_funding_at = col_date(format = "%m/%d/%Y")))
```

# data celaning

We take a quick look at the data
```{r}
skim(data)
```


We start by removing columns there are undefined when loading the dataset and columns which show the same things. We also remove variables like longtitude and latitude, which could be fun to work with later, but doesnt seem to bring much insight to the ongoing analysis. 

```{r}

data %<>%
  select(!c(`Unnamed: 0`, `Unnamed: 6`, state_code.1, object_id, avg_participants, has_roundA, has_roundB, has_roundC, has_roundD, zip_code, id, city, name, latitude, longitude, labels))

```

The data contains 33 numerical variables after the first cleaning, but 22 of those are dummy variables, which are variables we later on could try and see if our clusters are separated by. But as over main objective is to try and separate by the status column "closed or "acquired" we will now take a look an see, which variables seems to be correlated with the status column, starting with the jobtype related dummies, the state related dummies and lastly the three remaining dummies.

```{r, warning=FALSE}
jobs = data %>% 
  select(is_software, is_web, is_mobile, is_enterprise, is_advertising, is_gamesvideo, is_ecommerce, is_biotech, is_consulting, is_othercategory, status)

state = data %>% 
  select(is_CA, is_TX, is_MA, is_NY, is_otherstate, status)

dummies = data %>%
  select(is_top500, has_angel, has_VC, status)

par(mfrow=c(1,3))
jobs %>%
  select(status, is_numeric) %>%
  gather(variable, value, -status) %>%
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(status), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)

state %>%
  select(status, is_numeric) %>%
  gather(variable, value, -status) %>%
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(status), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)

dummies %>%
  select(status, is_numeric) %>%
  gather(variable, value, -status) %>%
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(status), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)

```

The plot of the job type related dummies shows, that there is almost no differences in how many firm closes and how many who becomes acquired according to job type.
  Looking at the state related dummies, the plot shows that a bit more firms from New York, Texas and California are being acquired than from the rest of the united states, which could indicate that they could be useful variables to use in our analysis. But you could also say, that the differences aren't big which would make them less influential in the analysis.
   Lastly in the plot of the three remaining dummy variables it is clear to see that whether or not the firm has been a top 500 upcoming firm substantially change the chance of closing or not. Whether they have VC or angel investor doesn't seem to influence the outcome in a big way. 

Because of the above we drop the state and job variables plus the VC and angel dummies, but we keep is_top200 and then we skim the data again.

```{r}
data %<>%
  select(-c(is_CA, is_TX, is_MA, is_NY, is_otherstate, is_software, is_web, is_mobile, is_enterprise, is_advertising, is_gamesvideo, is_ecommerce, is_biotech, is_consulting, is_othercategory, state_code, category_code, has_angel, has_VC))

skim(data)

```
We still have 8 numerical variables and 3 date variables we haven't taken a closer look at so that is what we gonna do now.

```{r, warning=FALSE}

data %>%
  select(status, is_numeric, !is_top500) %>%
  gather(variable, value, -status, -is_top500) %>%
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(status), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)

```

The above plot shows, that relationships, milestones, funding_total_usd, funding_rounds, closed at, both first and last milestone_year and last_funding_year seems to impact whether or not a firm gets acquired or not. 
    Closed_at will not be used because it contains almost 600 missing observations which is close to 66% of all observations. Founded at will not be used either simply because there doesnt seem to be that big of a difference in the density plots. And using both first and last milestone_year would maybe be irrelevant because they somehow nearly show the same thing. so we drop them. First and last funding_at doesn't show any significance either, so those also gets dropped.
    
```{r}
data%<>%
  select(!c(age_last_milestone_year, closed_at, age_first_funding_year,founded_at, first_funding_at, last_funding_at))
```

Now we are left with only our status variable and our 7 numerical variables of interest. Now we check for missing values in our remaning variables.

```{r, fig.height=7, fig.width=9}
data %>%
  select(everything()) %>%
  summarise_all(funs(sum(is.na(.))))
```

The above states that the age_first_milestone_year has 152 missing values. THe problem with this variable is, that if we remove those rows, we lose 16.4% of our observations, and we cant just replace the NA's with something else like zero, because that would just manipulate our data. So the solution we have come up with is to make them into intervals and then just pick one of the intervals to be used in our analysis. 

```{r}

#We make intervals for first milestone
data %<>%
  mutate(age_first_milestone_year= ifelse(age_first_milestone_year <0,"before year 0",ifelse(age_first_milestone_year <= 3 , "[0-3]", ifelse(age_first_milestone_year <= 6, "]3-6]", ifelse(age_first_milestone_year >6, "over 6",""))))) %>%
  mutate(age_first_milestone_year= replace_na(age_first_milestone_year, "no milestone"))

data %<>% 
  mutate("is_before_start" = as.numeric(age_first_milestone_year == "before year 0")) %>% 
  mutate("is_0:3" = as.numeric(age_first_milestone_year == "[0-3]")) %>% 
  mutate("is_3:6" = as.numeric(age_first_milestone_year == "]3-6]")) %>%
  mutate("is_6<" = as.numeric(age_first_milestone_year == "over 6")) %>% 
  mutate("is_no_milestone" = as.numeric(age_first_milestone_year == "no milestone"))

```

We pick the interval zero to three years, which means that firms achieved their first milestone when they were between 0 and 3 years old. We pick this interval because we wanna investigate whether it is positive or not in terms of getting acquired for a firm to get their first milestone quick eg. in the first couple of years.

And this conclude our variable selecting which now will be explained more thoroughly below.

Forklar hvad hver enkelt variabel er.




# EDA
Lav SIomns plot og tag dem der ser mest relevante ud

Funding rounds, total_funding_usd, milestones, relationships, is_top500, intervaller af age first milestone year ingen, 0-3, 3-6 og over 6.

```{r}

data %<>%
  filter(percent_rank(funding_total_usd) <0.99)
 
``` 

```{r, fig.height=12, fig.width=12, warning=FALSE}
data %>% 
  select(funding_total_usd, funding_rounds, relationships,age_first_funding_year, age_last_funding_year, milestones, status) %>%
  ggpairs(legend = 1,
          mapping = ggplot2::aes(colour=status, alpha = 0.5), 
          lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1))) +
  theme(legend.position = "bottom")

```
Ja vi kan droppe de to ringe.


Så laver vi nogle lækre summary stats
```{r}
data %>% 
  group_by(status) %>% 
  select(relationships, milestones, funding_rounds, funding_total_usd) %>% 
  summarise_if(is.numeric, mean, na.rm = TRUE)
```
Og så videre til PCA.

# Unsupervised ML

Should we scale the data?

```{r}
options(scipen = 999)

data_num = data %>% 
  select(funding_total_usd, funding_rounds, relationships,age_first_funding_year, age_last_funding_year, milestones)

s_deviation=apply(data_num,2, sd)

mean1=colMeans(data_num)

scale= as.data.frame(s_deviation, row.names = c("sd"))%>%
cbind(as.data.frame(mean1, row.names = "mean"))%>%
print()
```
We can se the data is gonna need scaling to peform pca, because our variables is not on the same scale. Namely funding_total_usd seems much higher.

```{r warning=FALSE}
pca_data = data %>% 
  select(funding_total_usd, funding_rounds, relationships, milestones, is_top500, `is_0:3`)

res_pca <- pca_data %>%
  PCA(scale.unit = TRUE, graph =FALSE)

```

```{r}
res_pca %>% 
  fviz_screeplot(addlabels = TRUE, 
                 ncp = 10, 
                 ggtheme = theme_gray())
```
We can see the elbow shows the optimal dimensions are two dimensions with almost 80% explained variance. Then we visualize our reduced data in two dimensions.

```{r, fig.height=7, fig.width=9, warning=FALSE}
res_pca %>%
  fviz_pca_biplot(alpha.ind = "cos2",
                  col.ind = "contrib",
                  gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                  geom = "point",
                  ggtheme = theme_gray())
```
From the plot we can see, that the x-axis split our six variables into two groups so to say. Where the ones involving funding, funding rounds, milestone and relationships are above the x-axis and our two variables involving the age of a firm during its first and last funding is below. 

```{r, fig.height=7, fig.width=9, warning=FALSE}
res_pca %>%
fviz_pca_biplot(alpha.ind = "cos2",
habillage = data %>% pull(status) %>% factor(),
addEllipses = TRUE,
geom = "point",
ggtheme = theme_gray())

```

## UMAP
```{r}
res_umap <- pca_data  %>%
umap(n_neighbors = 15,
     metric = "cosine",
min_dist = 0.01,
scale = TRUE)
```

```{r}
res_umap %>%
as_tibble() %>%
ggplot(aes(x = V1, y = V2, fill = data$status)) +
geom_point(shape = 21, alpha = 0.5)
```
## Clustering

```{r}
pca_data %>%
scale() %>%
fviz_nbclust(kmeans, method = "wss")
```
Seems like the optimal number of clusters is somewhere around 3 or 4 clusters, so we try both

```{r}
res_km3 <- pca_data %>%
scale() %>%
kmeans(centers = 3, nstart = 20)

res_km4 <- pca_data %>%
scale() %>%
kmeans(centers = 4, nstart = 20)
```


```{r}
par(mfrow=c(2,1))
res_km3 %>%
fviz_cluster(data = pca_data ,
ggtheme = theme_gray())

res_km4 %>%
fviz_cluster(data = pca_data ,
ggtheme = theme_gray())
```

```{r}
data[,"cluster3"] <- res_km3$cluster
data[,"cluster4"] <- res_km4$cluster
  
table(data$cluster3, data$category_code)
table(data$cluster3, data$status)
table(data$cluster3, data$state_code)

table(data$cluster4, data$category_code)
table(data$cluster4, data$status)
table(data$cluster4, data$state_code)
```
The clusters don't seem to be separated by either category_code, active or state_code


```{r}
data[,"pca1"] <- res_pca$ind$coord[,1]
data[,"pca2"] <- res_pca$ind$coord[,2]
```


```{r}
par(mfrow=c(2,1))
data %>%
ggplot(aes(x=pca1, y=pca2, color=factor(cluster3)))+
geom_point()
data %>%
ggplot(aes(x=pca1, y=pca2, color=factor(cluster4)))+
geom_point()
```
## Clustering after dimensionality reduction
```{r}
data_pca= data%>%
  select(pca1, pca2)

data_pca %>%
  scale() %>%
  fviz_nbclust(kmeans, method = "wss")
```

We can now see the elbow is formed at 3 clusters, so we use 3.

```{r}

res_km_pca1 <- data_pca %>% 
  scale() %>% 
  kmeans(centers = 4, nstart = 20)  
```

```{r}
res_km_pca1 
```

```{r,,fig.width=15,fig.height=10,fig.align='center'}
res_km_pca1 %>% 
  fviz_cluster(data = data_pca,
               ggtheme = theme_gray())  

#Supervised ML Classification of aquired and closed
```


# SML
```{r}
data %>% head()
```

```{r}
data %>% glimpse()

```


```{r}
data_sml= data %>%
  rename(y = status) %>%
  select(y, age_first_funding_year, age_last_funding_year, age_first_milestone_year, age_last_milestone_year, relationships, funding_rounds, funding_total_usd, milestones, is_top500, has_angel, is_CA, is_NY, is_MA, is_TX, is_software, is_web) 

glimpse(data_sml)
```
##EDA sml

```{r,fig.height=5,fig.width=12.5}
data_sml %>%
  gather(variable, value, -y) %>% # Note: At one point do pivot_longer instead
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(y), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)
```
## Data Preprocessing


### Training & Test split

```{r}
data_split <- initial_split(data_sml, prop = 0.75, strata = y)

data_train <- data_split  %>%  training()
data_test <- data_split %>% testing()
```


### Preprocessing recipe

Tror den sletter alle categoriske variable ved ikk om der skal laves en step_dummy for categroical?

```{r}

data_recipe <- data_train %>%
  recipe(y ~.) %>%
  step_log(funding_total_usd) %>% #Myb just delete this one
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  na.omit(all_predictors()) %>% #  knn inputation of missing values
  prep()
```

### Defining the models

#### Logistic Regression

```{r}
model_lg <- logistic_reg(mode = 'classification') %>%
  set_engine('glm', family = binomial) 
```

#### Decision tree

```{r}
model_dt <- decision_tree(mode = 'classification',
                          cost_complexity = tune(),
                          tree_depth = tune(), 
                          min_n = tune()
                          ) %>%
  set_engine('rpart') 
```

#### Extreme Gradient Boosted Tree (XGBoost)

```{r}
model_xg <- boost_tree(mode = 'classification', 
                       trees = 100,
                       mtry = tune(), 
                       min_n = tune(), 
                       tree_depth = tune(), 
                       learn_rate = tune()
                       ) %>%
  set_engine("xgboost") 
```

#### Define workflow

```{r}
workflow_general <- workflow() %>%
  add_recipe(data_recipe) 

workflow_lg <- workflow_general %>%
  add_model(model_lg)

workflow_dt <- workflow_general %>%
  add_model(model_dt)

workflow_xg <- workflow_general %>%
  add_model(model_xg)
```



### Hyperparameter Tuning

#### Validation Sampling (N-fold crossvlidation)

```{r}
data_resample <- data_train %>% 
  vfold_cv(strata = y,
           v = 3,
           repeats = 3)
```

#### Hyperparameter Tuning: Decision Tree

Need to find out what the error is here!

```{r}
tune_dt <-
  tune_grid(
    workflow_dt,
    resamples = data_resample,
    grid = 5
  )
```

```{r}
tune_dt %>% autoplot()
```

```{r}
best_param_dt <- tune_dt %>% select_best(metric = 'roc_auc')
best_param_dt
```

```{r}
tune_dt %>% show_best(metric = 'roc_auc', n = 1)
```


#### Hyperparameter Tuning: Random Forest



```{r}

tune_xg <-
  tune_grid(
    workflow_xg,
    resamples = data_resample,
    grid = 10
  )
```

```{r}
tune_xg %>% autoplot()
```

```{r}
best_param_xg <- tune_xg %>% select_best(metric = 'roc_auc')
best_param_xg
```

```{r}
tune_xg %>% show_best(metric = 'roc_auc', n = 1)
```


#### Fit models with tuned hyperparameters


```{r}
workflow_final_dt <- workflow_dt %>%
  finalize_workflow(parameters = best_param_dt)

workflow_final_xg <- workflow_xg %>%
  finalize_workflow(parameters = best_param_xg)
```

```{r}
fit_lg <- workflow_lg %>%
  fit(data_train)

fit_dt <- workflow_final_dt %>%
  fit(data_train)

fit_xg <- workflow_final_xg %>%
  fit(data_train)
```

#### Compare performance

```{r}
pred_collected <- tibble(
  truth = data_train %>% pull(y) %>% as.factor(),
  #base = mean(truth),
  lg = fit_lg %>% predict(new_data = data_train) %>% pull(.pred_class),
  dt = fit_dt %>% predict(new_data = data_train) %>% pull(.pred_class),
  xg = fit_xg %>% predict(new_data = data_train) %>% pull(.pred_class),
  ) %>% 
  pivot_longer(cols = -truth,
               names_to = 'model',
               values_to = '.pred')
```

```{r}
pred_collected %>% head()
```


```{r}
pred_collected %>%
  group_by(model) %>%
  accuracy(truth = truth, estimate = .pred) %>%
  select(model, .estimate) %>%
  arrange(desc(.estimate))
```

```{r}
pred_collected %>%
  group_by(model) %>%
  bal_accuracy(truth = truth, estimate = .pred) %>%
  select(model, .estimate) %>%
  arrange(desc(.estimate))
```

Surprisingly, here the less complex model seems to hve the edge!


#### Final prediction

So, now we are almost there. Since we know we will use the random forest, we only have to predict on our test sample and see how we fair...

```{r}
fit_last_dt <- workflow_final_dt %>% last_fit(split = data_split)
```

```{r}
fit_last_dt %>% collect_metrics()
```

#### Variable importance

```{r}
fit_last_dt %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 11)
```


```{r}
fit_xg %>%
  pull_workflow_fit() %>%
  vip::vip(num_features = 20)
```