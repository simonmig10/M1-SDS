---
title: "Assignment 1"
output:
  pdf_document: default
  html_document: default
---

#Before we start

Running libaries 

```{r message=FALSE}
library(tidyverse)

library(lubridate)

library(magrittr)
```


Loading data 

```{r message=FALSE , warning= FALSE}
trips= read_csv("https://sds-aau.github.io/SDS-master/M1/data/trips.csv")
colnames(trips)[1] = "obs"

people= read_csv("https://sds-aau.github.io/SDS-master/M1/data/people.csv")
colnames(people)[1] = "obs"

country= read_csv("https://sds-aau.github.io/SDS-master/M1/data/countrylist.csv")

```


Looking at the data: 

```{r}
glimpse(trips)

glimpse(people)

glimpse(country)
```



#Preprocessing

**Trips: transform dates into timestamps (note: in Python, you will have to coerce errors for faulty dates)**

First we remove the missing values in the columns "date_end" and "date_start", for this we use the drop_na function.
We use the mutate function to create a new variable to show the timestamps, in the mutate function we use as.numeric to change the class of the variable from Date to numeric. We use the as.POSIXct function to tell how to make this convertion, where we specify the column "date_end" with the format "Y" (Year in 4 digits), "m" (Month in decimal number) and "d" (Day of the month in decimal number). The function as.POSIXct gives the values as number of seconds since January 1, 1970.

```{r}

trips %>%
  filter(is.na(date_end) | is.na(date_start))

trips %<>%
  drop_na(date_end, date_start)%>%
  mutate(date_end_time= as.numeric(as.POSIXct(date_end, format="%Y-%m-%d")), date_start_time=as.numeric(as.POSIXct(date_start, format="%Y-%m-%d")))%>%
select(date_end_time, date_start_time, everything())

trips %>%
  filter(is.na(date_end_time) | is.na(date_start_time))

```


**Calculate trip duration in days (you can use loops, list comprehensions or map-lambda-functions (python) to create a column that holds the numerical value of the day. You can also use the datetime package.)**

We again use the mutate function to create a new variable called "trip_duration", this variable shows the trip duration in days. First the date_start_times (in seconds) is subtracted from the date_end_times (in seconds). Now we have the trip durration in seconds, we then devide by 86400 ($60**60**24$) to make it into days.

```{r}
trips %<>% 
  mutate(trip_duration= date_end_time- date_start_time, trip_duration_days= trip_duration/86400)%>%
   select(trip_duration_days, everything())

trips
```




**Filter extreme (fake?) observations for durations as well as dates - start and end (trips that last 234565 days / are in the 17th or 23rd century) The minimum duration of a trip is 1 day! Hint: use percentiles/quantiles to set boundaries for extreme values - between 1 and 97, calculate and store the boundaries before subsetting. Rhint: Use percent_rank(as.numeric(variable)) to create percentiles**

First we use the mutate function to create new variables for "trip_duration_days", "date_end_time" and "date_star_time" that shows the percentile rank for the values in these. For this we use the percent_rank function. We can now use the filter function to remove the outliers from the data. First we set the minimum duration of a trip to 1 day and keep the values up to the 98. percentile. In date_end we remove values up to the 1. percentile and keep the values up to the 99. percentile. 

To set these values we have used the summarise function to see the min and max values of the variables, and for the different percentiles we can now see if the values are realsitic. 

```{r}

trimmed_trips=trips %>%
  mutate(trip_duration_days_rank= percent_rank(trip_duration_days), date_end_time_rank= percent_rank(date_end_time), date_start_time_rank= percent_rank(date_start_time))%>%
  filter(trip_duration_days >= 1 & trip_duration_days_rank < 0.98, date_end_time_rank > 0.01 & date_end_time_rank < 0.99)
  
trimmed_trips %>%
  summarise(min_start_date= min(date_start), max_start_date= max(date_start), min_end_date= min(date_end), max_end_date= max(date_end), min_trip_durr= min(trip_duration_days), max_trip_durr= max(trip_duration_days))

```



**Join the countrylist data to the trips data-frame using the countrycode as a key e. [Only for python users ] Set DateTime index as the start date of a trip**

We use the left_join function to join the country datasets observations into the trips dataset. The left_join function is coded to look for matching values in the "country_code" column and the "alpha_2" column. 

We can see there is 1705 NA's by using the count function. This is mainly duo to the fact that the UK in the country dataset had the aplha_2 value of "GB" and in the trips dataset it had the country_code "UK". After inserting the UK row into the country dataset there's only 44 NA's left in the "region" column, this is because there have not been a matching value in the "alpha_2" column in the country dataset.

```{r}

country_new=country%>%
  filter(!is.na(alpha_2))

trimmed_no_na=trimmed_trips%>%
  filter(!is.na(country_code))

trimmed_no_na%>%
  left_join(country_new, by= c("country_code" = "alpha_2"))%>%
  count(region)

UK = data.frame("UK", "Europe", "Northern Europe")
names(UK) = c("alpha_2", "region", "sub_region")
country_new = rbind(country_new, UK)

trimmed_no_na%>%
  left_join(country_new, by= c("country_code" = "alpha_2")) %>%
  count(region)

trimmed_no_na%>%
  left_join(country_new, by= c("country_code" = "alpha_2"))%>%
  filter(is.na(region))

trimmed_trips_2 = trimmed_no_na%>%
  left_join(country_new, by= c("country_code" = "alpha_2"))
```

#people 

**How many people have a least a “High School” diploma? Hint: For this calculation remove missing value-rows or fill with “False”.**
We have made this assignment using 2 ways. The reason we did this, was that we thought that we presume that everyone has at least a high school diploma since one cannot get a bachelors degree without getting a high school. But in the first method we just search for the strings containing high school.

1) We use the filter function to filter all the strings containing "High School" in the education_raw category and count all these. This gives us that 130 have at least high school diploma.

2) First we drop the NAs. Thereafter we count the remaining observations as high school is presumed to be the lowest education in the category. From this we see that 451 people have at least a High School diploma.

```{r}
people %>%
  filter(grepl("High School", education_raw)) %>%
  count()

people %>%
 drop_na(education_raw) %>%
  count()
```

**How many “Startup Founders” have attained a “Master’s Degree”? Bonus: compared to people who don’t have a formal higher education (e.g. by using the “False” occurrences)?**
Here we use the grepl command to filter by both the work_raw and education_raw. The grepl lets us filter by strings containing certain text. By doing this we get that 53 startup founders have attained a Master's Degree. By adding an "!" before the second grepl we can reverse the function so that we get all startup founders without a Master's degree. This shows us that 668 startup founders does not have a master's degree.

```{r}
people %>%
  filter(grepl("Startup Founder", work_raw)) %>%
  filter(grepl("Master",education_raw)) %>%
  count()

people %>%
  filter(grepl("Startup Founder", work_raw)) %>%
  filter(!grepl("Master",education_raw)) %>%
  count()

```

**Who is the person with a Master’s Degree that has the highest number of followers? Bonus: Explore the individual further, what else can you find out?**
To get the results we first filtered in the education_raw category by strings containing "Master's Degree". Afterwards we arranged by followers in descending order, so that the top result will be the person with most followers. Thereafter we write the "head(1)" function which excludes all results not in the top of the list. In the end we use the select function to exclude the ID of the person, because this is not relevant.
The result shows us, that the person with the most followers is @levelsio. He has 2182 followers. He follows 353. He is a software Dev, a startup founder and a creative. Furthermore he has a high school diploma, a bachelor's degree and a master's degree.

```{r}
people %>%
  filter(grepl("Master's Degree", education_raw)) %>%
  arrange(desc(followers)) %>%
  head(1) %>%
  select(-obs)

```

#Trips

**Which country received the highest number of trips? – And which the lowest?**
To find out which country received the most and fewest trips, we just count the number of times each country shows up in the trips datasets and then filter for the maximum and minimum value. The result shows that the most visited country is the United States with 6970, and 56 country share the price of being the lowest visited destination with one trip. Around thirteen of the countries who has been visited the lowest amount of times contained numerical values in the country column, so those have been dropped in the second part of this assignment. This leaves us with the result that 44 countries share the price of being the lowest visited destination with one trip.

```{r}
w_numbers_trips = trimmed_trips %>%
  count(country, sort =TRUE) %>%
  filter(n %in% c(min(n), max(n)))

w_numbers_trips

wo_numbers_trips = w_numbers_trips %>%
  filter(!grepl("1|2|3|4|5|6|7|8|9|0", country))

wo_numbers_trips
```

**Which region received the highest number of trips in 2017? Use the start of trips as a time reference.**
To access regions we need to join our trips and country datasets, but we already did that earlier and saved is as our new trips dataset, so no need to do that again. Then we drop the trips which has no region becuase they are not of significance to us. Then we filter for trips that started in 2017 and count the number of times each region was visited, and then we end up with, that the most visited region in 2017 was Europe with 4785 trips. 

```{r}
trimmed_trips_2 %>%
  drop_na(region) %>%
  filter(date_start >= "2017-01-01", date_start <= "2017-12-31") %>%
  count(region, sort = TRUE)

```


**Which country in “Western Europe” did travelers spent least time? – Provide visualization**
To figure out which country in Western Europe travelers spent the least time we first drop the NAs from both sub_region and trip_duration_days as observations with no values in are of no interest to us. Then we filter for the sub_region Western Europe, group_by country and summarise the total number og days spent in each Western european country. The below results will be presented visually.

```{r}
plot_trips = trimmed_trips_2 %>%
  drop_na(sub_region, trip_duration_days) %>%
  filter(sub_region == "Western Europe") %>% 
  group_by(country) %>%
  summarise(total_time = sum(trip_duration_days))


```

```{r}
plot_trips %>% 
  ggplot(aes(x = total_time, y = reorder(country, -total_time), fill= country)) + 
  geom_col() + scale_fill_manual(values = rainbow(9)) +
  ggtitle("Total number of days spent in Western European countries") +
  labs(x = "Number of days", y = "Countries")

```
The above bar chart show visually the number of days spent in each western european country, where Lichtenstein clearly is where travelers have spent the least amount of time (1).

**Do nomad Startup Founders tend to have shorter or longer trips on average?**
On average every person who is or has been a Startup Founder spend 19.62 days per trip vs. 20.57 days per trip for all other occupations. We are not sure if this is the result you guys wanted, because we had a hard time understanding what we would be comparing the startup founders average trip length to.

```{r}
trimmed_trips_2 %>% 
  left_join(people, by = c("username" = "username")) %>% 
 filter(grepl("Startup Founder", work_raw)) %>% 
  summarise(avg_time = mean(trip_duration_days))

trimmed_trips_2 %>% 
  left_join(people, by = c("username" = "username")) %>% 
 filter(!grepl("Startup Founder", work_raw)) %>% 
  summarise(avg_time = mean(trip_duration_days))

```


**visualize over-time median trip duration overall (bonus: and split by world-region) The plot will look weird ^^. PyHint: Resample by week (‘W’) and calculate the size of observations. RHint: Use the floor_date function to reset dates by week.**
So we are not sure what we should use the floor_date function to, instead we just made our trip_duration_days column to trip_duration_week by dividing it with 7. Beforehand we dropped the NAs and then we grouped by region to calculate the median for every region and then we plotted it. This just doesnt visualise it over time, this only visualize the median by region over the entire time period. The assignment doesnt state in which interval to check the overall development in the median, so below we plot the median by region every year in the dataset.

```{r}
trimmed_trips_2 %>%
  drop_na(region) %>% 
  mutate(trip_duration_week = trip_duration_days/7) %>%
  group_by(region) %>% 
  summarise(median = median(trip_duration_week)) %>% 
  ggplot(aes(x = region, y = median, fill = region)) + geom_col() + scale_fill_manual(values = rainbow(5))
  
```

We do this by making a column representing which year the trip took place, then grouping both by region and year, to get the medians and then plotting them using ggplot. We see that the median changes over time and mostly in the region of Oceania.

```{r}
trimmed_trips_2 %>% 
  mutate(year = format(as.Date(trimmed_trips_2$date_start, format="%Y-%m-%d"),"%Y")) %>% 
  drop_na(region, year) %>% 
  mutate(trip_duration_week = trip_duration_days/7) %>%
  group_by(region, year) %>% 
  summarise(median = median(trip_duration_week)) %>% 
  filter(median <= 10) %>%
  ggplot(aes(x = year, y = median, color = region, group = region)) + geom_line(size = 1) + scale_color_manual(values = rainbow(5))

```